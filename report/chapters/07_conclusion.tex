%%Conclusion (chpt6)
\chapter{Conclusion} %5

In this master's thesis, we achieved our goal of allowing true 3-D SLAM. We are able to build a 3D map based on observations by the monocular camera. To do so, we successfully:
\begin{itemize}
\item Researched the state of the art for monocular visual SLAM
\item Redesigned the keypoint acquisition step to use a more effective hybrid of keypoint tracking and detection
\item Implemented a state-of-the art method for optimal correction when triangulating keypoints
\item Implemented a bundle adjustment step that re-optimizes the map with new information
\item Implemented a mapping strategy that decides when to create keyframe, and when to re-optimize the map with global bundle adjustment
\end{itemize}


The code is now able to build a 3D map using a monocular camera, without making any assumptions regarding the location of keypoints. Using this map, we have shown that the drone can estimate its position very accurately, although at a low frequency. We are able to solve this frequency problem by fusing the visual pose estimation with the IMU, which is available at a much higher frequency, using the pose fusion algorithm created last year.\\

The full mapping algorithm works well for short distances, and we demonstrated that it is conceptually possible for it to work for long trajectories. Unfortunately, due to the unavailable sensors when out of flight, we were not able to show that our full mapping algorithm works during long trajectories. We did show that it could theoretically work during long trajectories by showing that it works when we replace the mapping strategy by a human decision. We are persuaded that if we had access to an ultrasonic sensor, the full mapping algorithm would work, without a human decision maker.\\

We will look at the two main limitations of the current code, and at some possible solutions to these limitations. Then we will look at some ideas to make the current implementation perform better. Finally, we will look at some new directions the UCL's drone project could take in the future years, in particular, what is made possible now that we implemented 3D mapping.\\


\section{Limitations}
The main limitations of our solution, is that with no ultrasound, the mapping algorithm fails on long trajectories, and that the speed of exploration is limited by the time required to run bundle adjustment. We will look at some ways to overcome these limitations.

\subsection{Failure on long trajectories}
In chapter \ref{chpt:results} we were not able to show that our algorithm works during long trajectories. However, we did show that it works on short trajectories, and that if we replace the mapping strategy by a human decision of when to create keyframes (to be able to tell the drone its altitude when keyframes are created), it works quite well. For these reasons, we are convinced that with an available ultrasonic sensor,  the current algorithm would work on long trajectories, and would be able to carry out loop closure.

\subsubsection{Possible solutions}
It would greatly ease development to have access to all sensor data individually, and at any time. This is impossible with the Parrot AR.Drone, because the software running inside is closed-source. Only a change in hardware can really solve this problem, but several solutions are possible:
\begin{itemize}
\item Use another drone, one that has open software, and that is modular so that sensors can be added and removed at will.
\item Attach a new, independent, small ultrasonic sensor to the bottom of the drone, that can communicate its altitude to the drone at all times.
\item Install an external sensor, such as a Kinect\texttrademark , to measure the position of the drone, and use it to simulate the ultrasonic sensor. This would have the added benefit of giving a ground truth of the drone's position at all times, even during displacements.
\end{itemize}

\subsubsection{Computation time of bundle adjustment}
The time taken by bundle adjustment varies widely depending on the number of keyframes considered, the number of points considered, the quality of the initial solution, and the number of bad point matches. In the worst cases, we found that local bundle adjustment could take more than \SI{1}{\second}, which severely impacts the speed at which the drone can explore, because local bundle adjustment is run every time a keyframe is created, and keyframes have to be created frequently enough for all points to be seen from at least two different positions.

\subsubsection{Possible solutions}
Some of the possible ways to reduce the computation time of bundle adjustment are:
\begin{itemize}
\item Further reduce the map to keep only the best points.
\item Reduce the number of keyframes considered during local bundle adjustment. Currently, more keyframes are required to reduce the scale drift (by setting two keyframes constant), but if we had an ultrasound available, it could be used to prevent scale drift, reducing the need for many keyframes during local bundle adjustment.
\item Use a graphics card to parallelize computations.
\end{itemize}

\section{Possible improvements of our implementation}
Here are some ideas that could improve the results of our work, but that weren't tried out:

\subsubsection{Further sanitize the map}
As we could see on the various images of point clouds, many points remain that are behind the wall of the room in which the tests were carried out, which indicates that at least these points don't correspond to real world locations. One idea we did not try out is to remove landmarks that have been present in the map for some time (to be defined) but that were never (or rarely) inliers for RANSAC during localization. This would indicate that they are not useful, and can be removed.

\subsubsection{Changing the type of keypoints}
SIFT keypoints with a SURF detector were chosen last year during one of the master's theses that was written that year. The student who made this choice found that it has the best compromise between performance and complexity. Their have been some important changes to the implementation since then, so it might be interesting to do some experiments with different types of keypoints. For example, ORB keypoints seem to have worked well for other, similar projects.

\subsubsection{A survival-of-the-fittest type strategy for keyframes}
The idea for a survival-of-the-fittest type strategy for keyframes comes from ORB-SLAM \cite{orbslam}. With this strategy, we would create many more keyframes, and then remove all but the "best" ones. This way, we could aim for keyframes placed in space such that there is enough overlap between them, but also such that they are not too close to each other. This would make the need for deciding in advance when to create keyframes less important, moving the decision to later, when there is more information available.

\subsubsection{A separate node for global bundle adjustment}
Because bundle adjustment takes a long time, it is done in a separate node. The mapping node sends part of the map to the bundle adjustment node, which optimizes this part of the map and sends it back to the mapping node. When the mapping node decides to run global bundle adjustment, it simply sends the entire map to the bundle adjustment node. Having a separate node just for global bundle adjustment would allow to run global bundle adjustment in the background permanently. 


\section{Next steps for this project}
There are many different directions that can be taken for the next steps of this project. Here, we highlight some of those that would build on what was achieved in this master's thesis.

\subsubsection{A higher level representation of the map}
Having a real 3D map can be useful for a variety of applications. The current point-cloud configuration of our map well suited for using it to localize the drone by solving the PnP problem, but less suited for other tasks. The point cloud can be transformed to higher level representations, such as a voxel grid. Voxel stands for volumetric pixel, and is a division of 3D space into small units. Such a representation would allow to be able to tell for each point in space whether there is an object there or not, which can then be used for obstacle avoidance. Alternatively, it could be interesting to use the point cloud to determine where walls are, in order to create a human-readable map of a building.\\

\subsubsection{Collaborative mapping}
One of the goals of this project is to enable a group of drones to collaborate. In the previous years, basic communication between drones was implemented, for example allowing them to use a map created by another drone. A more ambitious objective would be for a group of drones to simultaneously build a common map. The implementation of bundle adjustment takes a step in this direction, as we could imagine matching keypoints between keyframes created by two separate drones, resulting in a common map. This would not be trivial, however, as we would need to keep multiple separate maps until there is some overlap between them.

\subsubsection{An Extended Kalman Filter}
The pose estimation coming from PnP alone is not enough to stabilize the drone in flight, as it comes at a frequency too low for the controller. The current pose fusion scheme incorporates the IMU's pose estimation, but does so  with a very basic method. An EKF would significantly improve the accuracy of the fused pose estimation, and allow for a much better controller.

\subsubsection{Hardware change}
A hardware change is always an attractive idea. One of the reasons for a change would be to use a more modular drone, to be able to freely add and remove sensors. Changing the current monocular camera to a stereo or RGB-D camera is tempting, because it lifts the scale uncertainty, and allows to map points from single views, but it should be kept in mind that a monocular camera also has some advantages:
\begin{itemize}
\item They are much more common, making our code useable on a wide variety of devices.
\item The scale ambiguity can also be a monocular camera's strong point: it means that they can be used at any scale, using the same camera, and the same techniques. This is not the case for stereo and RGB-D cameras, which have a maximal and minimal working range.
\item A considerable part of the current code was written specifically for monocular cameras, and would have to be redone in the event of a change.
\end{itemize}